{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ogb_hOZ7wNGd",
    "outputId": "772d9830-eb5e-483d-cf20-a2063894ae89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.9.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already up-to-date: lxml in /usr/local/lib/python3.6/dist-packages (4.5.1)\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install --upgrade beautifulsoup4\n",
    "!sudo pip install --upgrade lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE9r4tOJKlRq"
   },
   "outputs": [],
   "source": [
    "#%run Pre_ini.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lW9aH5N-wNJS"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup,Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbsqg-vBv-dI"
   },
   "outputs": [],
   "source": [
    "def extract_text(soup):\n",
    "        \n",
    "    blacklist = set([\n",
    "        '[document]',\n",
    "        \n",
    "        #'header',\n",
    "        'dialog',\n",
    "        'html',\n",
    "        'input',\n",
    "        \n",
    "        'script',\n",
    "        'noscript',\n",
    "        'applet',\n",
    "        'embed',  \n",
    "        'object',\n",
    "        'param',\n",
    "        \n",
    "        \n",
    "        'head',\n",
    "        'meta',\n",
    "        'base',\n",
    "        'basefont',\n",
    "\n",
    "        'style',\n",
    "    ])\n",
    "    \n",
    "    def is_comment(element): \n",
    "        return isinstance(element, Comment)\n",
    "        \n",
    "    for comment in soup.find_all(text=is_comment):\n",
    "        comment.extract()\n",
    "    \n",
    "    \n",
    "    ans=''\n",
    "    for text in soup.find_all(text=True):\n",
    "        if text.parent.name in blacklist:\n",
    "            continue\n",
    "        ans+=process_string(text)+' '\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7gIC6yKwDIa"
   },
   "outputs": [],
   "source": [
    "no_head=0\n",
    "no_body=0\n",
    "\n",
    "beg=time()\n",
    "\n",
    "\n",
    "i=0\n",
    "er=0\n",
    "with ZipFile('data.zip') as myzip:\n",
    "    files=myzip.namelist()\n",
    "    with open('forward_docs.txt','w') as out:\n",
    "        for file_name in files:\n",
    "            with myzip.open(file_name) as f:\n",
    "                url=f.readline().decode('utf-8').strip()\n",
    "                doc_id=get_id[url]\n",
    "                raw_html=f.read()\n",
    "                try:\n",
    "                    html=raw_html.decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    html=raw_html.decode('latin-1')\n",
    "                    er+=1\n",
    "            \n",
    "            out.write(str(doc_id)+'\\t')\n",
    "            soup=BeautifulSoup(html)\n",
    "            if soup.head:\n",
    "                tmp=soup.head.find('meta',{'name':'description'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.find('meta',{'name':'author'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.find('meta',{'name':'keywords'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.title\n",
    "                out.write(process_string(tmp.text) if tmp is not None else '')\n",
    "                out.write('\\t')\n",
    "            else:\n",
    "                out.write('\\t'*4)\n",
    "                no_head+=1\n",
    "\n",
    "            if soup.body:\n",
    "                out.write(extract_text(soup.body))\n",
    "            else:\n",
    "                no_body+=1\n",
    "            out.write('\\n')\n",
    "\n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "            i+=1\n",
    "\n",
    "print(no_head,no_body,er)\n",
    "print((time()-beg)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(soup):\n",
    "        \n",
    "    blacklist = set([\n",
    "        '[document]',\n",
    "        \n",
    "        #'header',\n",
    "        'dialog',\n",
    "        'html',\n",
    "        'input',\n",
    "        \n",
    "        'script',\n",
    "        'noscript',\n",
    "        'applet',\n",
    "        'embed',  \n",
    "        'object',\n",
    "        'param',\n",
    "        \n",
    "        \n",
    "        'head',\n",
    "        'meta',\n",
    "        'base',\n",
    "        'basefont',\n",
    "\n",
    "        'style',\n",
    "    ])\n",
    "    \n",
    "    def is_comment(element): \n",
    "        return isinstance(element, Comment)\n",
    "        \n",
    "    for comment in soup.find_all(text=is_comment):\n",
    "        comment.extract()\n",
    "    \n",
    "          \n",
    "    classes=[\n",
    "             set(['a']),\n",
    "             set(['h1','h2']),\n",
    "             set(['h3','h4','h5','h6']),\n",
    "             set(['strong','b','em','i','u','dl','ol','ul'])\n",
    "    ]\n",
    "    ans=['' for i in range(len(classes)+1)]\n",
    "    for text in soup.find_all(text=True):\n",
    "        if text.parent.name in blacklist:\n",
    "            continue\n",
    "        ind=0\n",
    "        for cls in classes:\n",
    "            if text.parent.name in cls:\n",
    "                ans[ind]+=string_lem(text)+' '\n",
    "                break\n",
    "            ind+=1\n",
    "        if ind==len(classes):\n",
    "            ans[ind]+=string_lem(text)+' '\n",
    "    return reduce(lambda x,y:x+'\\t'+y,ans)\n",
    "    \n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_head=0\n",
    "no_body=0\n",
    "\n",
    "beg=time()\n",
    "\n",
    "\n",
    "i=0\n",
    "er=0\n",
    "with ZipFile('data.zip') as myzip:\n",
    "    files=myzip.namelist()\n",
    "    with open('all_docs.txt','w') as out:\n",
    "        for file_name in files:\n",
    "            with myzip.open(file_name) as f:\n",
    "                url=f.readline().decode('utf-8').strip()\n",
    "                doc_id=get_id[url]\n",
    "                raw_html=f.read()\n",
    "                try:\n",
    "                    html=raw_html.decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    html=raw_html.decode('latin-1')\n",
    "                    er+=1\n",
    "            \n",
    "            out.write(str(doc_id)+'\\t')\n",
    "            soup=BeautifulSoup(html)\n",
    "            if soup.head:\n",
    "                tmp=soup.head.find('meta',{'name':'description'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.find('meta',{'name':'author'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.find('meta',{'name':'keywords'})\n",
    "                out.write(process_string(tmp['content']) if tmp is not None and tmp.has_attr('content') else '')\n",
    "                out.write('\\t')\n",
    "\n",
    "                tmp=soup.head.title\n",
    "                out.write(process_string(tmp.text) if tmp is not None else '')\n",
    "                out.write('\\t')\n",
    "            else:\n",
    "                out.write('\\t'*4)\n",
    "                no_head+=1\n",
    "\n",
    "            if soup.body:\n",
    "                out.write(extract_text(soup.body))\n",
    "            else:\n",
    "                no_body+=1\n",
    "                out.write('\\t'*4)\n",
    "            out.write('\\n')\n",
    "\n",
    "            if i%100==0:\n",
    "                print(i)\n",
    "            i+=1\n",
    "\n",
    "print(no_head,no_body,er)\n",
    "print((time()-beg)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCoqoCNAPZlf"
   },
   "outputs": [],
   "source": [
    "IDF=defaultdict(int)\n",
    "with open('forward_docs.txt','r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        words=' '.join(line.strip('\\n').split('\\t')[1:]).replace('.',' ').split()\n",
    "        for word in set(words):\n",
    "            IDF[word]+=1\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "with open('IDF.pickle','wb') as out:\n",
    "    pickle.dump(IDF,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnWFHhJj98AB"
   },
   "outputs": [],
   "source": [
    "IDF_lem=defaultdict(int)\n",
    "with open('forward_docs.txt','r') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        words=' '.join(line.strip('\\n').split('\\t')[1:]).replace('.',' ').split()\n",
    "        for word in set(map(lambda x:add_cash(x),\n",
    "                            words)):\n",
    "            IDF_lem[word]+=1\n",
    "        if i%500==0:\n",
    "            print(i)\n",
    "with open('IDF_lem.pickle','wb') as out:\n",
    "    pickle.dump(IDF_lem,out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Worker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
