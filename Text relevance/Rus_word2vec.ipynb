{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4h_3c3WJeUM"
   },
   "outputs": [],
   "source": [
    "!sudo pip install --upgrade gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZpJU7KCuJhyp"
   },
   "outputs": [],
   "source": [
    "!pip install ufal.udpipe\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBzVtKHCJjTq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import wget\n",
    "import re\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "\n",
    "'''\n",
    "Этот скрипт принимает на вход необработанный русский текст \n",
    "(одно предложение на строку или один абзац на строку).\n",
    "Он токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
    "На выход подаётся последовательность разделенных пробелами лемм с частями речи \n",
    "(\"зеленый_NOUN трамвай_NOUN\").\n",
    "Их можно непосредственно использовать в моделях с RusVectōrēs (https://rusvectores.org).\n",
    "Примеры запуска:\n",
    "echo 'Мама мыла раму.' | python3 rus_preprocessing_udpipe.py\n",
    "zcat large_corpus.txt.gz | python3 rus_preprocessing_udpipe.py | gzip > processed_corpus.txt.gz\n",
    "'''\n",
    "\n",
    "\n",
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma\n",
    "\n",
    "\n",
    "def list_replace(search, replacement, text):\n",
    "    search = [el for el in search if el in text]\n",
    "    for c in search:\n",
    "        text = text.replace(c, replacement)\n",
    "    return text\n",
    "\n",
    "\n",
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    # Если частеречные тэги не нужны (например, их нет в модели), выставьте pos=False\n",
    "    # в этом случае на выход будут поданы только леммы\n",
    "    # По умолчанию знаки пунктуации вырезаются. Чтобы сохранить их, выставьте punct=True\n",
    "\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(token, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "        if pos in entities:\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n",
    "\n",
    "\n",
    "# URL of the UDPipe model\n",
    "udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(udpipe_filename):\n",
    "    print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "    wget.download(udpipe_model_url)\n",
    "\n",
    "print('\\nLoading the model...', file=sys.stderr)\n",
    "model = Model.load(udpipe_filename)\n",
    "process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UFQrmS94JtR6"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('rus_corp.zip', 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model_word = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "rus_word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
