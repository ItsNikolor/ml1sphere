{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFUg8qxpth_W"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade deeppavlov\n",
    "!pip install --upgrade transformers\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-L1xmwswV3zM",
    "outputId": "0f48930e-bfae-48d1-9723-5d30e394df15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504M/504M [00:22<00:00, 22.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "bert=model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "sentence_embeddings = bert.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mt_J6EaAkPeI"
   },
   "outputs": [],
   "source": [
    " %run Pre_ini.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnSG8tSiiWzt"
   },
   "outputs": [],
   "source": [
    "queries=dict()\n",
    "with open('queries.fixed.txt') as f:\n",
    "    for line in f:\n",
    "        query_id,words=line.split('\\t')\n",
    "        queries[int(query_id)]=process_string(words).strip().replace('.',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTRj37dXQxv4"
   },
   "outputs": [],
   "source": [
    "with open('super_query_ext.pickle','rb') as f:\n",
    "    query_ext=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylcMBk3xO6vE"
   },
   "outputs": [],
   "source": [
    "candidates,reverse_candidates=generate_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "PKFGQMSKXPIm",
    "outputId": "9c2c3a55-8152-4785-bdb1-8a12b564abe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "queries_emb=dict()\n",
    "i=0\n",
    "for query_id,query in queries.items():\n",
    "    queries_emb[query_id]=bert.encode([query])\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggFR1_I5P5VI"
   },
   "outputs": [],
   "source": [
    "tmp_query_ext=dict()\n",
    "for key in query_ext:\n",
    "    tmp_query_ext[key]=set(filter(lambda word: word not in stopwords,\n",
    "                                  map(lambda x:x[1],query_ext[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnWF5rVuv4sp"
   },
   "outputs": [],
   "source": [
    "file_names=['divide_cos0.txt','divide_cos1.txt','divide_cos2.txt','divide_cos3.txt']\n",
    "for n in range(0,4):\n",
    "    i1=n*10000\n",
    "    i2=(n+1)*10000\n",
    "    with open(file_names[n],'w') as out:\n",
    "        with open('forward_docs.txt','r') as f:\n",
    "            beg=time()\n",
    "            for i,line in enumerate(f):\n",
    "                if i>=i2 or i<i1:\n",
    "                    continue\n",
    "                doc_id,desc,author,keywords,title,word=line.strip('\\n').split('\\t')\n",
    "\n",
    "                texts=list(filter(lambda x:x and any(map(lambda x:x!=' ',x)),\n",
    "                                  title.split('.')+author.split('.')+\n",
    "                                  desc.split('.')+keywords.split('.')+word.split('.')\n",
    "                                 ))\n",
    "                \n",
    "                def divide_text(text,n):\n",
    "                        t=text.split()\n",
    "                        return [' '.join(t[i*n:(i+1)*n]) for i in range(len(t)//n+int(len(t)%n!=0))]\n",
    "                texts=[new_text for text in texts for new_text in divide_text(text,20)]\n",
    "                \n",
    "                texts_len=len(texts)\n",
    "                old_texts=enumerate(texts)\n",
    "\n",
    "                for query_id in reverse_candidates[int(doc_id)]:\n",
    "                    query = queries[query_id]\n",
    "                    s=tmp_query_ext[query_id]\n",
    "                    query_emb=queries_emb[query_id][0]\n",
    "                    \n",
    "                    \n",
    "                    texts=list(filter(lambda text:any(map(lambda word:add_cash(word) in s,\n",
    "                                                        text[1].split())),old_texts))\n",
    "                    \n",
    "                    texts=list(map(lambda text:(text[0],' '.join(text[1].split())),texts))\n",
    "                    \n",
    "                    s=set()\n",
    "                    def addset(s,text):\n",
    "                        if text in s:\n",
    "                            return False\n",
    "                        else:\n",
    "                            s.add(text)\n",
    "                            return True\n",
    "                    texts=list(filter(lambda text:addset(s,text[1]),texts))\n",
    "\n",
    "                    out.write(doc_id+'\\t'+query)\n",
    "                    if texts:\n",
    "                        sent_embs=bert.encode(list(map(lambda item:item[1],texts)))\n",
    "                        distances = scipy.spatial.distance.cdist([query_emb], sent_embs, \"cosine\")[0]\n",
    "                        \n",
    "                        for ind in np.argsort(distances):\n",
    "                            out.write('\\t'+texts[ind][1]+','+str(distances[ind])+','+str(1-texts[ind][0]/texts_len))\n",
    "                    out.write('\\n')\n",
    "                    if i%10==0:\n",
    "                        print(i)\n",
    "\n",
    "\n",
    "            print((time()-beg)/(i+1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Text_relevance_passage3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
