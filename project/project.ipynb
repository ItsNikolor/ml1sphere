{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.cluster as sc\n",
    "import sklearn.metrics as smt\n",
    "from scipy.cluster import hierarchy\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "# Plotting config\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymorphy2\n",
    "#pip install -U pymorphy2-dicts-ru\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "\n",
    "class Clustering(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"\n",
    "    Implement clustering algorithm according \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric='euclidean', n_clusters=1,linkage='average',distance_threshold=None):\n",
    "        \"\"\"\n",
    "        metric - string with name of metric, for example, euclidean\n",
    "        Please add necessary algoritm parameters to class constructor.\n",
    "        \"\"\"\n",
    "        self.n_clusters=n_clusters\n",
    "        self.distance_threshold=distance_threshold\n",
    "        if distance_threshold is not None and n_clusters is not None:\n",
    "            print('If distance_threshold is not None - n_clusters shoud be None')\n",
    "            raise\n",
    "            \n",
    "        self.dist=None\n",
    "        self.clusters=None\n",
    "        self.ans=None\n",
    "        self.Z=None\n",
    "        \n",
    "        \n",
    "        if metric=='euclidean':\n",
    "            self.metric=self.__eucl\n",
    "        elif metric=='manhattan':\n",
    "            self.metric=self.__manh\n",
    "        elif metric=='cosine':\n",
    "            self.metric=self.__cosine\n",
    "        elif metric=='precomputed':\n",
    "            self.metric=lambda x:x\n",
    "        else:\n",
    "            print('invalid metric')\n",
    "            raise\n",
    "        \n",
    "        if linkage=='single':\n",
    "            self.link=self.__single\n",
    "        elif linkage=='complete':\n",
    "            self.link=self.__compl\n",
    "        elif linkage=='average':\n",
    "            self.link=self.__aver\n",
    "        else:\n",
    "            print('invalid linkage')\n",
    "            raise\n",
    "    \n",
    "    def __eucl(self,X):\n",
    "        return np.sqrt(((X[...,np.newaxis]-X.T[np.newaxis])**2).sum(axis=1))\n",
    "    \n",
    "    def __manh(self,X):\n",
    "        return (np.abs(X[...,np.newaxis]-X.T[np.newaxis])).sum(axis=1)    \n",
    "    \n",
    "    def __cosine(self,X):\n",
    "        tmp=(X[...,np.newaxis]*X.T[np.newaxis]).sum(axis=1)\n",
    "        norm=np.sqrt(tmp.diagonal())\n",
    "        mask=norm==0\n",
    "        norm[mask]=1\n",
    "        return 1-tmp/norm[np.newaxis]/norm[...,np.newaxis]\n",
    "    \n",
    "    def __single(self,u,v):\n",
    "        mask=self.dist[u]>self.dist[v]\n",
    "        \n",
    "        self.dist[u,mask]=self.dist[v,mask]\n",
    "        self.dist[:,u]=self.dist[u]\n",
    "\n",
    "        self.dist[v]=np.inf\n",
    "        self.dist[:,v]=np.inf\n",
    "        \n",
    "        self.dist[u,u]=np.inf\n",
    "            \n",
    "    def __compl(self,u,v):\n",
    "        mask=self.dist[u]<self.dist[v]\n",
    "        \n",
    "        self.dist[u][mask]=self.dist[v][mask]\n",
    "        self.dist[:,u]=self.dist[u]\n",
    "        \n",
    "        self.dist[v]=np.inf\n",
    "        self.dist[:,v]=np.inf\n",
    "        \n",
    "        self.dist[u,u]=np.inf\n",
    "    \n",
    "    def __aver(self,u,v):\n",
    "        ai=len(self.clusters[u])\n",
    "        aj=len(self.clusters[v])\n",
    "        N=ai+aj\n",
    "        ai/=N\n",
    "        aj/=N\n",
    "        \n",
    "        self.dist[u]=ai*self.dist[u]+aj*self.dist[v]\n",
    "        self.dist[:,u]=self.dist[u]\n",
    "        \n",
    "        self.dist[v]=np.inf\n",
    "        self.dist[:,v]=np.inf\n",
    "        \n",
    "        self.dist[u,u]=np.inf\n",
    "    \n",
    "    def fit(self,x):\n",
    "        self.dist=self.metric(x)\n",
    "        n_clusters=N=cur_cluster=len(x)\n",
    "        self.ans=np.arange(N)\n",
    "        \n",
    "        self.clusters=dict()\n",
    "        for i in range(n_clusters):\n",
    "            self.clusters[i]=[i]\n",
    "            self.dist[i,i]=np.inf\n",
    "        \n",
    "        if self.n_clusters==1:\n",
    "            self.Z=np.empty((N-1,4))\n",
    "            cur_z=0\n",
    "        \n",
    "        while(n_clusters!=self.n_clusters):\n",
    "            pos=self.dist.argmin()\n",
    "            distanse=self.dist.ravel()[pos]\n",
    "            if self.distance_threshold is not None:\n",
    "                if distanse>self.distance_threshold:\n",
    "                    break\n",
    "            u=pos//N\n",
    "            v=pos%N\n",
    "            \n",
    "            max_cluster=u if len(self.clusters[u])>=len(self.clusters[v]) else v\n",
    "            min_cluster=u if max_cluster==v else v\n",
    "            u,v=max_cluster,min_cluster\n",
    "            \n",
    "            if self.n_clusters==1:\n",
    "                self.Z[cur_z]=[self.ans[self.clusters[u][0]],self.ans[self.clusters[v][0]],\n",
    "                               max(0,distanse),len(self.clusters[u])+len(self.clusters[v])]\n",
    "                cur_z+=1\n",
    "                \n",
    "            self.link(u,v)\n",
    "            \n",
    "            for i in self.clusters[v]:\n",
    "                self.clusters[u].append(i)\n",
    "\n",
    "            self.ans[self.clusters[u]]=cur_cluster\n",
    "\n",
    "            cur_cluster+=1\n",
    "            \n",
    "            n_clusters-=1\n",
    "            if(n_clusters==1):\n",
    "                break\n",
    "        return self\n",
    "    \n",
    "    def predict(self):\n",
    "        return self.ans\n",
    "        \n",
    "    def fit_predict(self, x):\n",
    "        \"\"\"\n",
    "        Use data matrix x to compute model parameters and predict clusters\n",
    "        \"\"\"\n",
    "        self.fit(x)\n",
    "        return self.predict()\n",
    "    \n",
    "    def plot_dendrogram(self):\n",
    "        \"\"\"\n",
    "        Try to visualize our data\n",
    "        \"\"\"\n",
    "        if self.Z is not None:\n",
    "            hierarchy.dendrogram(self.Z)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Tree must be full')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Гератор новых батчей для обучения\n",
    "    X          - матрица объекты-признаки\n",
    "    y_batch    - вектор ответов\n",
    "    shuffle    - нужно ли случайно перемешивать выборку\n",
    "    batch_size - размер батча ( 1 это SGD, > 1 mini-batch GD)\n",
    "    Генерирует подвыборку для итерации спуска (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    \n",
    "    idx=np.arange(y.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    for i in range(0,y.shape[0],batch_size):\n",
    "        X_batch = X[idx[i:min(i+batch_size,X.shape[0])]]\n",
    "        y_batch = y[idx[i:min(i+batch_size,X.shape[0])]]\n",
    "        yield (X_batch, y_batch)\n",
    "\n",
    "# Теперь можно сделать генератор по данным ()\n",
    "#  my_batch_generator = batch_generator(X, y, shuffle=True, batch_size=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Вычисляем значение сигмоида.\n",
    "    X - выход линейной модели\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Your code Here\n",
    "    e=np.exp(x)\n",
    "    sigm_value_x=e/(1+e)\n",
    "    sigm_value_x[sigm_value_x==0]=0.0000000000000001\n",
    "    sigm_value_x[sigm_value_x==1]=0.9999999999999999\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01, max_epoch=10, model_type='lin_reg'):\n",
    "        \"\"\"\n",
    "        batch_generator -- функция генератор, которой будем создавать батчи\n",
    "        C - коэф. регуляризации\n",
    "        alpha - скорость спуска\n",
    "        max_epoch - максимальное количество эпох\n",
    "        model_type - тим модели, lin_reg или log_reg\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем функцию потерь по батчу \n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "        if self.model_type=='lin_reg':\n",
    "            loss=((y_batch-self.predict(X_batch))**2).sum()/X_batch.shape[0]\n",
    "        else:\n",
    "            loss=-(np.dot(y_batch[np.newaxis],np.log2(self.predict(X_batch))[...,np.newaxis])\\\n",
    "                    +np.dot((1-y_batch)[np.newaxis],np.log2(1-self.predict(X_batch))[...,np.newaxis]))[0,0]\\\n",
    "                    /X_batch.shape[0]\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Считаем  градиент функции потерь по батчу (то что Вы вывели в задании 1)\n",
    "        X_batch - матрица объекты-признаки по батчу\n",
    "        y_batch - вектор ответов по батчу\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        \"\"\"\n",
    "\n",
    "        loss_grad=np.dot((self.predict(X_batch)-y_batch)[np.newaxis],\\\n",
    "                         np.hstack((np.ones((X_batch.shape[0],1)),X_batch))).ravel()/X_batch.shape[0]\n",
    "        if self.model_type=='lin_reg':\n",
    "            loss_grad*=2\n",
    "        else:\n",
    "            from math import log\n",
    "            loss_grad/=log(2)\n",
    "            \n",
    "        loss_grad[1:]+=2*self.weights[1:]/self.C\n",
    "        \n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        \"\"\"\n",
    "        Обновляем вектор весов\n",
    "        new_grad - градиент по батчу\n",
    "        \"\"\"\n",
    "        self.weights-=self.alpha*new_grad\n",
    "    \n",
    "    def fit(self, X, y,batch_size=1):\n",
    "        '''\n",
    "        Обучение модели\n",
    "        X - матрица объекты-признаки\n",
    "        y - вектор ответов\n",
    "        '''\n",
    "        \n",
    "        # Нужно инициализровать случайно веса\n",
    "        np.random.seed(0)\n",
    "        self.weights = np.random.ranf(size=X.shape[1]+1).astype(np.float64)\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X,y,batch_size=batch_size)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]\n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                # Подумайте в каком месте стоит посчитать ошибку для отладки модели\n",
    "                # До градиентного шага или после\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Предсказание класса\n",
    "        X - матрица объекты-признаки\n",
    "        Не забудте тип модели (линейная или логистическая регрессия)!\n",
    "        '''\n",
    "        X=X.reshape(-1,self.weights.shape[0]-1)\n",
    "        if self.model_type=='lin_reg':\n",
    "            y_hat=np.dot(X,self.weights[1:][...,np.newaxis]).ravel()+self.weights[0]\n",
    "        else:\n",
    "            y_hat=sigmoid(np.dot(X,self.weights[1:][...,np.newaxis]).ravel()+self.weights[0])\n",
    "            \n",
    "        # Желательно здесь использовать матричные операции между X и весами, например, numpy.dot \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv',encoding=\"utf8\") as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print(len(doc_to_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Удаляю лишнее и разбиваю предложения на слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_words={'на','в','и','или','от','для','про','под','об','над','за','у','с','через','при','перед','от','о','по','на','к',\n",
    "            'из','до','без'}\n",
    "\n",
    "for key,line in doc_to_title.items():\n",
    "    line=line.lower()\n",
    "    tmp=list(line)\n",
    "    for i, c in enumerate(line):\n",
    "        if(not(c.isalpha())):\n",
    "            tmp[i]=' '\n",
    "    tmp=list(filter(lambda x: x not in delete_words and len(x)>1,''.join(tmp).split()))\n",
    "    doc_to_title[key]=tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализую слова**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for key,lst in doc_to_title.items():\n",
    "    doc_to_title[key]=[morph.parse(x)[0].normal_form for x in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore = 0.7197093096103245\n"
     ]
    }
   ],
   "source": [
    "link='single'\n",
    "y_true=[]\n",
    "Y_pred1=[]\n",
    "\n",
    "for gr,value in traingroups_titledata.items():\n",
    "    words=dict()\n",
    "    for i in value:\n",
    "        for word in i[1]:\n",
    "            if word not in words:\n",
    "                words[word]=len(words)\n",
    "\n",
    "    x=np.zeros((len(value),len(words)+1))\n",
    "    y=np.empty(len(value)).astype(int)\n",
    "    \n",
    "    \n",
    "    d=defaultdict(int)\n",
    "    for ind,i in enumerate(value):\n",
    "        \n",
    "        for word in i[1]:\n",
    "            x[ind,words[word]]+=1\n",
    "            if i[2]:\n",
    "                d[word]+=1\n",
    "        y[ind]=i[2]\n",
    "        y_true.append(i[2])\n",
    "        \n",
    "   \n",
    "    \n",
    "    old_x=x\n",
    "    s=x.sum(axis=1)<=1\n",
    "    x[s]=0\n",
    "    \n",
    "    сustum_aggl = Clustering(metric='cosine',n_clusters=None,linkage=link,distance_threshold=0.55)\n",
    "    y_pred=сustum_aggl.fit_predict(x)\n",
    "    \n",
    "    tmp=y_pred\n",
    "    \n",
    "    un=np.unique(y_pred)\n",
    "    count=(y_pred[...,np.newaxis]==un[np.newaxis]).sum(axis=0)\n",
    "    mask=((y_pred[...,np.newaxis]==un[count<=4][np.newaxis]).sum(axis=1)).astype(bool)\n",
    "\n",
    "    \n",
    "    y_pred[mask]=0\n",
    "    y_pred[~mask]=1\n",
    "    Y_pred1+=list(y_pred)\n",
    "    \n",
    "print('fscore =',smt.f1_score(y_true,Y_pred1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore = 0.7187928669410151\n"
     ]
    }
   ],
   "source": [
    "y_true=[]\n",
    "Y_pred2=[]\n",
    "\n",
    "for gr,value in traingroups_titledata.items():\n",
    "    words=dict()\n",
    "    for i in value:\n",
    "        for word in i[1]:\n",
    "            if word not in words:\n",
    "                words[word]=len(words)\n",
    "\n",
    "    x=np.zeros((len(value),len(words)+1))\n",
    "    y=np.empty(len(value)).astype(int)\n",
    "    \n",
    "    \n",
    "    d=defaultdict(int)\n",
    "    for ind,i in enumerate(value):\n",
    "        \n",
    "        for word in i[1]:\n",
    "            x[ind,words[word]]=1\n",
    "            if i[2]:\n",
    "                d[word]+=1\n",
    "        y[ind]=i[2]\n",
    "        y_true.append(i[2])\n",
    "    \n",
    "    s=x.sum(axis=1)<=1\n",
    "    x[s]=0\n",
    "    \n",
    "    old_x=x\n",
    "    \n",
    "    inter=np.dot(x,x.T)\n",
    "    s=x.sum(axis=1)\n",
    "    union=s[...,np.newaxis]+s[np.newaxis]\n",
    "    union-=inter\n",
    "    union[union==0]=1\n",
    "    x=inter/union\n",
    "    x=1-x\n",
    "    \n",
    "    model=sklearn.cluster.DBSCAN(eps=0.7,min_samples=5,metric='precomputed',algorithm='brute').fit(x)\n",
    "    y_pred=model.labels_\n",
    "\n",
    "    tmp=y_pred\n",
    "    \n",
    "   \n",
    "    \n",
    "    mask=y_pred==-1\n",
    "    \n",
    "    y_pred[mask]=0\n",
    "    y_pred[~mask]=1\n",
    "    Y_pred2+=list(y_pred)\n",
    "\n",
    "print('fscore =',smt.f1_score(y_true,Y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 10) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "N1=5\n",
    "N2=5\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    value=docs\n",
    "    \n",
    "    words=dict()\n",
    "    for i in value:\n",
    "        y_train.append(i[2])\n",
    "        for word in i[1]:\n",
    "            if word not in words:\n",
    "                words[word]=len(words)\n",
    "            \n",
    "    x=np.zeros((len(value),len(words)+1))\n",
    "\n",
    "    for ind,i in enumerate(value):\n",
    "        for word in i[1]:\n",
    "            x[ind,words[word]]+=1\n",
    "            \n",
    "    \n",
    "    count_x=x\n",
    "    bool_x=(x>=1).astype(int)\n",
    "    \n",
    "    s=bool_x.sum(axis=1)<=1\n",
    "    \n",
    "    bool_x[s]=0\n",
    "    count_x[s]=0\n",
    "    \n",
    "    inter=np.dot(bool_x,bool_x.T)\n",
    "    s=bool_x.sum(axis=1)\n",
    "    union=s[...,np.newaxis]+s[np.newaxis]\n",
    "    union-=inter\n",
    "    union[union==0]=1\n",
    "    x=inter/union\n",
    "    x=1-x\n",
    "    \n",
    "    \n",
    "    l=[(0.01,5),(0.05,5),(0.1,5),(0.01,4),(0.05,4),\n",
    "       (0.1,4),(0.2,5),(0.25,5),(0.2,4),(0.25,4),(0.4,5),(0.4,4),(0.65,5),(0.75,5),(0.85,5),(1,1)]\n",
    "    \n",
    "    for (eps,min_s) in l:\n",
    "        model=sklearn.cluster.DBSCAN(eps=eps,min_samples=min_s,metric='precomputed',algorithm='brute').fit(x)\n",
    "        y_pred=model.labels_\n",
    "        mask=y_pred==-1\n",
    "        if y_pred[~mask].shape[0]>=N1:\n",
    "            break\n",
    "    \n",
    "    if (eps,min_s)==l[-1]:\n",
    "        x=np.zeros((len(value),N1)) \n",
    "    else:\n",
    "        target_x =bool_x[~mask]\n",
    "        inter=np.dot(bool_x,target_x.T)\n",
    "        union=bool_x.sum(axis=1)[...,np.newaxis]+target_x.sum(axis=1)[np.newaxis]\n",
    "        union-=inter\n",
    "        union[union==0]=1\n",
    "        x=inter/union\n",
    "        \n",
    "        sort_ind=np.argsort(x,axis=1)\n",
    "        x=np.take_along_axis(x, sort_ind, axis=1)[:,::-1]\n",
    "        x=x[:,:N1]\n",
    "    \n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        all_dist = []\n",
    "\n",
    "        title_words = set(title)\n",
    "        words=title_words\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "\n",
    "            _, title_j, _ = docs[j]\n",
    "            title_words_j = set(title_j)\n",
    "\n",
    "            words_j=title_words_j\n",
    "            if len(words)==1:\n",
    "                all_dist.append(0)\n",
    "            else:\n",
    "                inter=len(words.intersection(words_j))\n",
    "                all_dist.append(0 if inter==0 else inter/(len(words)+len(words_j)-inter))\n",
    "\n",
    "\n",
    "        tmp=sorted(all_dist, reverse=True)[0:N2]\n",
    "        tmp+=list(x[k])\n",
    "        X_train.append(tmp)\n",
    "        \n",
    "        \n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        groups_train.append(new_group)\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.hstack([X_train,np.array(Y_pred1)[...,np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_all=[]\n",
    "for value in np.unique(groups_train):\n",
    "    S_all.append(groups_train==value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "minlos=1e10\n",
    "for alpha in [0.1,0.05,0.01,0.2,0.3]:\n",
    "    for c in [2*alpha/0.1,2*alpha/0.05]:\n",
    "        for model_type in ['log_reg']:\n",
    "            for max_epoch in [50,100,150,200]:\n",
    "                loss=0\n",
    "                for s in S_all[:10]:\n",
    "                    model=MySGDClassifier(batch_generator,alpha=alpha,C=c,model_type=model_type,max_epoch=max_epoch)\n",
    "                    model.fit(X[~s],y_train[~s],batch_size=11690)\n",
    "                    loss+=model.calc_loss(X[s],y_train[s])\n",
    "                if loss<minlos:\n",
    "                    minlos=loss\n",
    "                    best_alpha=alpha\n",
    "                    best_c=c\n",
    "                    best_max_epoch=max_epoch\n",
    "                    best_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_alpha=0.1 best_c=4.0 best_epoch=50\n"
     ]
    }
   ],
   "source": [
    "print(f'best_alpha={best_alpha} best_c={best_c} best_epoch={best_max_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha=0.1\n",
    "best_c=4\n",
    "best_max_epoch=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MySGDClassifier(batch_generator,alpha=best_alpha,C=best_c,model_type='log_reg',max_epoch=best_max_epoch)\n",
    "model.fit(X,y_train,batch_size=11690)\n",
    "best_f=0\n",
    "for th in np.linspace(0.1,1,1000,endpoint=False):\n",
    "    y=model.predict(X)>=th\n",
    "    f=smt.f1_score(y_train,y)\n",
    "    if f>best_f:\n",
    "        best_f=f\n",
    "        best_th=th\n",
    "best_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_f=0.726380042462845 best_th=0.40869999999999995\n"
     ]
    }
   ],
   "source": [
    "print(f'best_f={best_f} best_th={best_th}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.zeros((0,1))\n",
    "for s in S_all:\n",
    "    y_true=np.vstack([y_true,y_train[s][...,np.newaxis]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7247645576336385\n"
     ]
    }
   ],
   "source": [
    "f=0\n",
    "models=[]\n",
    "y_pred=np.zeros((0,1))\n",
    "for s in S_all:\n",
    "    model=MySGDClassifier(batch_generator,alpha=best_alpha,C=best_c,model_type='log_reg',max_epoch=best_max_epoch)\n",
    "    model.fit(X[~s],y_train[~s],batch_size=11690)\n",
    "    y=model.predict(X[s])>=best_th\n",
    "    y_pred=np.vstack([y_pred,y[...,np.newaxis]])\n",
    "    models.append(model)\n",
    "    \n",
    "print(smt.f1_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "pair_id=[]\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))\n",
    "    pair_id.append(new_doc['pair_id'])\n",
    "pair_id=np.array(pair_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627,)\n"
     ]
    }
   ],
   "source": [
    "link='single'\n",
    "\n",
    "Y_pred1_test=[]\n",
    "\n",
    "\n",
    "for gr,value in testgroups_titledata.items():\n",
    "    words=dict()\n",
    "    for i in value:\n",
    "        for word in i[1]:\n",
    "            if word not in words:\n",
    "                words[word]=len(words)\n",
    "\n",
    "    x=np.zeros((len(value),len(words)+1))\n",
    "    \n",
    "    \n",
    "   \n",
    "    for ind,i in enumerate(value):\n",
    "        \n",
    "        for word in i[1]:\n",
    "            x[ind,words[word]]=1\n",
    "            \n",
    "    s=x.sum(axis=1)<=1\n",
    "    x[s]=0\n",
    "    \n",
    "    сustum_aggl = Clustering(metric='cosine',n_clusters=None,linkage=link,distance_threshold=0.55)\n",
    "    y_pred=сustum_aggl.fit_predict(x)\n",
    "    \n",
    "   \n",
    "    un=np.unique(y_pred)\n",
    "    count=(y_pred[...,np.newaxis]==un[np.newaxis]).sum(axis=0)\n",
    "    mask=((y_pred[...,np.newaxis]==un[count<=4][np.newaxis]).sum(axis=1)).astype(bool)\n",
    "  \n",
    "    \n",
    "    y_pred[mask]=0\n",
    "    y_pred[~mask]=1\n",
    "    Y_pred1_test+=list(y_pred)\n",
    "Y_pred1_test=np.array(Y_pred1_test)\n",
    "print(Y_pred1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 10) (16627,)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "groups_test = []\n",
    "N1=5\n",
    "N2=5\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    value=docs\n",
    "    \n",
    "    words=dict()\n",
    "    for i in value:\n",
    "        for word in i[1]:\n",
    "            if word not in words:\n",
    "                words[word]=len(words)\n",
    "\n",
    "    x=np.zeros((len(value),len(words)+1))\n",
    "\n",
    "    for ind,i in enumerate(value):\n",
    "        for word in i[1]:\n",
    "            x[ind,words[word]]+=1\n",
    "            \n",
    "    \n",
    "    count_x=x\n",
    "    bool_x=(x>=1).astype(int)\n",
    "    \n",
    "    s=bool_x.sum(axis=1)<=1\n",
    "    \n",
    "    bool_x[s]=0\n",
    "    count_x[s]=0\n",
    "    \n",
    "    inter=np.dot(bool_x,bool_x.T)\n",
    "    s=bool_x.sum(axis=1)\n",
    "    union=s[...,np.newaxis]+s[np.newaxis]\n",
    "    union-=inter\n",
    "    union[union==0]=1\n",
    "    x=inter/union\n",
    "    x=1-x\n",
    "    \n",
    "    l=[(0.01,5),(0.05,5),(0.1,5),(0.01,4),(0.05,4),\n",
    "       (0.1,4),(0.2,5),(0.25,5),(0.2,4),(0.25,4),(0.4,5),(0.4,4),(0.65,5),(0.75,5),(0.85,5),(1,1)]\n",
    "    \n",
    "    for (eps,min_s) in l:\n",
    "        model=sklearn.cluster.DBSCAN(eps=eps,min_samples=min_s,metric='precomputed',algorithm='brute').fit(x)\n",
    "        y_pred=model.labels_\n",
    "        mask=y_pred==-1\n",
    "        if y_pred[~mask].shape[0]>=N1:\n",
    "            break\n",
    "    \n",
    "    if (eps,min_s)==l[-1]:\n",
    "        x=np.zeros((len(value),N1))\n",
    "    else:\n",
    "        target_x =bool_x[~mask]\n",
    "        inter=np.dot(bool_x,target_x.T)\n",
    "        union=bool_x.sum(axis=1)[...,np.newaxis]+target_x.sum(axis=1)[np.newaxis]\n",
    "        union-=inter\n",
    "        union[union==0]=1\n",
    "        x=inter/union\n",
    "        \n",
    "        sort_ind=np.argsort(x,axis=1)\n",
    "        x=np.take_along_axis(x, sort_ind, axis=1)[:,::-1]\n",
    "        x=x[:,:N1]\n",
    "    \n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        all_dist = []\n",
    "\n",
    "        title_words = set(title)\n",
    "        words=title_words\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "\n",
    "            _, title_j = docs[j]\n",
    "            title_words_j = set(title_j)\n",
    "            \n",
    "            words_j=title_words_j\n",
    "            if len(words)==1:\n",
    "                all_dist.append(0)\n",
    "            else:\n",
    "                inter=len(words.intersection(words_j))\n",
    "                all_dist.append(0 if inter==0 else inter/(len(words)+len(words_j)-inter))\n",
    "\n",
    "        tmp=sorted(all_dist, reverse=True)[0:N2]\n",
    "        tmp+=list(x[k])\n",
    "        X_test.append(tmp)\n",
    "        \n",
    "        \n",
    "        \n",
    "    for i in range(len(x)):\n",
    "        groups_test.append(new_group)\n",
    "        \n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.hstack([X_test,np.array(Y_pred1_test)[...,np.newaxis]])\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627,)\n"
     ]
    }
   ],
   "source": [
    "model=best_model\n",
    "y_test=model.predict(X_test)>=best_th\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ans.csv','w',encoding=\"utf8\") as f:\n",
    "    print('pair_id,target',file=f)\n",
    "    for i,pair in enumerate(pair_id):\n",
    "        print(pair,f',{y_test[i]}',file=f,sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Финальный fscore = 0.71887**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
